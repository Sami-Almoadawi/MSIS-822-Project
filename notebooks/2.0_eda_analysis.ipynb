# Phase 2: Data Preprocessing & Exploratory Data Analysis (EDA)
# Task 2.2:
# Conduct a thorough EDA to determine linguistic patterns and differences between the two classes

!pip install xlsxwriter wordcloud matplotlib seaborn nltk python-docx openpyxl --quiet

import pandas as pd
import numpy as np
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from IPython.display import display
import io
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from docx.oxml import OxmlElement
import warnings

# Ignore warnings
warnings.filterwarnings("ignore")

# Download NLTK data
nltk.download('stopwords', quiet=True)

# Download Arabic Font
!wget -O NotoNaskhArabic-Regular.ttf https://cdn.jsdelivr.net/gh/notofonts/notofonts.github.io/fonts/NotoNaskhArabic/unhinted/ttf/NotoNaskhArabic-Regular.ttf > /dev/null 2>&1

# Font Setup
import matplotlib.font_manager as fm
font_path = 'NotoNaskhArabic-Regular.ttf'
prop = fm.FontProperties(fname=font_path)

# ==========================================
# --- Helper Functions ---
# ==========================================

def add_styled_table(doc, df, title, color_header='2E75B6', color_row='D9E8F5'):
    """Adds a professionally styled table to the Word document."""
    if title:
        doc.add_heading(title, level=3)

    # Create table
    table = doc.add_table(rows=len(df)+1, cols=len(df.columns))
    table.style = 'Table Grid'

    # Header
    for i, col_name in enumerate(df.columns):
        cell = table.rows[0].cells[i]
        cell.text = str(col_name)
        tcPr = cell._element.get_or_add_tcPr()
        tcVAlign = OxmlElement('w:shd')
        tcVAlign.set(qn('w:fill'), color_header)
        tcPr.append(tcVAlign)
        for paragraph in cell.paragraphs:
            for run in paragraph.runs:
                run.font.bold = True
                run.font.color.rgb = RGBColor(255, 255, 255)
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER

    # Rows
    for row_idx, row_data in enumerate(df.values, 1):
        for col_idx, value in enumerate(row_data):
            cell = table.rows[row_idx].cells[col_idx]
            if isinstance(value, float):
                cell.text = f"{value:.2f}"
            else:
                cell.text = str(value)

            # Alternating colors
            tcPr = cell._element.get_or_add_tcPr()
            tcVAlign = OxmlElement('w:shd')
            if row_idx % 2 == 0:
                tcVAlign.set(qn('w:fill'), 'F5F5F5')
            else:
                tcVAlign.set(qn('w:fill'), color_row)
            tcPr.append(tcVAlign)
            cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph("\n")

def add_plot_to_word(doc, plt_obj, title=""):
    """Saves the current matplotlib figure to the Word document."""
    if title:
        doc.add_paragraph(title, style='Caption')

    memfile = io.BytesIO()
    plt_obj.savefig(memfile, format='png', bbox_inches='tight')
    doc.add_picture(memfile, width=Inches(6))
    memfile.close()
    doc.add_paragraph("\n")

def display_colored_table(df, title=""):
    """Displays table in notebook."""
    styled_df = df.style.set_table_styles([
        {'selector': 'th', 'props': [('background-color', '#2c3e50'), ('color', 'white'), ('text-align', 'center')]},
        {'selector': 'td', 'props': [('text-align', 'center'), ('border', '1px solid #ccc')]}
    ])
    if title:
        print("\n" + title)
    display(styled_df)

# Initialize Word Document
doc = Document()
doc.add_heading('Exploratory Data Analysis (EDA) Report', 0)

# ==========================================
# --- Data Loading & Preparation ---
# ==========================================

excel_file = 'preprocessing_results.xlsx'

try:
    excel = pd.ExcelFile(excel_file)
    sheet_names = excel.sheet_names
except FileNotFoundError:
    print(f"Error: The file '{excel_file}' was not found. Please upload it.")
    sheet_names = []

frames = []
if sheet_names:
    for sheet in sheet_names:
        df = pd.read_excel(excel_file, sheet_name=sheet)

        # Determine Split Name
        split_part = sheet.split('_')[0]
        if split_part == 'by':
            split_name = 'by_polishing'
        elif split_part == 'from':
            if 'title_and_content' in sheet:
                split_name = 'from_title_and_content'
            else:
                split_name = 'from_title'
        else:
            split_name = split_part

        # Determine Text Type
        text_type = 'Human' if 'original_abstract' in sheet else 'AI'

        df['Split'] = split_name
        df['Text Type'] = text_type
        frames.append(df)

    df_all = pd.concat(frames, ignore_index=True)
    df_all = df_all.rename(columns={'Original': 'Original Text', 'After Preprocessing': 'Text After Processing'})

    # ==========================================
    # 1. Statistical Analysis (Sheets 1 & 2)
    # ==========================================
    doc.add_heading('1. Statistical Analysis', level=1)

    # Metrics
    df_all['Average Word Length'] = df_all['Text After Processing'].apply(
        lambda x: np.mean([len(word) for word in str(x).split()]) if isinstance(x, str) and len(x.split()) > 0 else 0)

    def avg_sentence_len(text):
        if not isinstance(text, str): return 0
        sentences = re.split(r'[.!؟،\n\r]', text)
        sentence_lengths = [len(s.strip().split()) for s in sentences if s.strip()]
        return np.mean(sentence_lengths) if sentence_lengths else 0
    df_all['Average Sentence Length'] = df_all['Text After Processing'].apply(avg_sentence_len)

    def ttr(text):
        words = str(text).split()
        return len(set(words)) / max(1, len(words))
    df_all['Type-Token Ratio'] = df_all['Text After Processing'].apply(ttr)

    # Sheet 1: Statistical Summary
    stat_summary = df_all.groupby(['Split', 'Text Type']).agg({
        'Average Word Length': 'mean',
        'Average Sentence Length': 'mean',
        'Type-Token Ratio': 'mean'
    }).reset_index()

    # Sheet 2: Global Averages
    mean_summary = stat_summary.groupby('Text Type')[['Average Word Length', 'Average Sentence Length', 'Type-Token Ratio']].mean().reset_index()

    # Display & Add to Word
    display_colored_table(stat_summary, "Summary statistics")
    add_styled_table(doc, stat_summary, "Summary Statistics")

    display_colored_table(mean_summary, "Global averages")
    add_styled_table(doc, mean_summary, "Global Averages")

    # Charts
    measure_names = {
        'Average Word Length': 'Average Word Length',
        'Type-Token Ratio': 'Type-Token Ratio'
    }
    for measure, ylabel in measure_names.items():
        plt.figure(figsize=(10,6))
        sns.barplot(data=stat_summary, x='Split', y=measure, hue='Text Type', palette='Set2', edgecolor='black')
        plt.title(f'{ylabel} by Split & Text Type')
        plt.tight_layout()
        add_plot_to_word(doc, plt, f'{ylabel} Bar Chart')
        plt.show()

    # ==========================================
    # 2. Text Length Analysis (Sheet 5)
    # ==========================================
    # We calculate this here to ensure we have the data for Sheet 5
    df_all['Original Text Length'] = df_all['Original Text'].apply(lambda x: len(str(x)))
    df_all['Processed Text Length'] = df_all['Text After Processing'].apply(lambda x: len(str(x)))
    df_all['Compression Ratio'] = (1 - df_all['Processed Text Length'] / df_all['Original Text Length']).clip(lower=0)

    length_summary = df_all.groupby(['Split', 'Text Type']).agg({
        'Original Text Length': 'mean',
        'Processed Text Length': 'mean',
        'Compression Ratio': 'mean'
    }).reset_index()

    # ==========================================
    # 3. Lexical Analysis (Sheets 3 & 4)
    # ==========================================
    doc.add_heading('2. Lexical Analysis', level=1)

    stop_ar = set(stopwords.words("arabic"))
    df_all['function_words_count'] = df_all['Text After Processing'].apply(lambda x: sum(1 for w in str(x).split() if w in stop_ar))
    df_all['punctuation_count'] = df_all['Original Text'].apply(lambda x: len(re.findall(r'[.,،؛:؛!\-؟"\'“”«»()]', str(x))))

    keywords = ['ان', 'جمع', 'بحث', 'الي', 'درس', 'علي']
    for kw in keywords:
        df_all[kw+'_count'] = df_all['Text After Processing'].apply(lambda t: str(t).count(kw))

    # Sheet 3: Lexical Analysis
    lexical_means = df_all.groupby(['Split', 'Text Type']).agg({
        'function_words_count': 'mean',
        'punctuation_count'   : 'mean',
        **{f"{kw}_count": 'sum' for kw in keywords}
    }).reset_index()

    # Sheet 4: Keywords (Sums by Text Type)
    # We need to map columns back to keyword names for cleaner sheet
    keyword_cols = [kw+'_count' for kw in keywords]
    keyword_summary = df_all.groupby('Text Type')[keyword_cols].sum().reset_index()
    keyword_summary.columns = ['Text Type'] + keywords # Rename to simple keyword names

    add_styled_table(doc, lexical_means, "Lexical Analysis")

    # Lexical Charts
    cols_to_plot = ['function_words_count', 'punctuation_count'] + [kw+'_count' for kw in keywords[:2]] # Plot first 2 kw for brevity
    for col in cols_to_plot:
        plt.figure(figsize=(10,6))
        sns.barplot(data=lexical_means, x='Split', y=col, hue='Text Type', palette='coolwarm')
        plt.title(f"{col} Comparison")
        plt.tight_layout()
        add_plot_to_word(doc, plt, f"{col} Comparison")
        plt.show()

    # ==========================================
    # 4. Visualization & N-grams (Sheet 6)
    # ==========================================
    doc.add_heading('3. Visualizations', level=1)

    # Word Clouds
    for split in df_all['Split'].unique():
        for ttype in ['Human', 'AI']:
            subset = df_all[(df_all['Split']==split) & (df_all['Text Type']==ttype)]
            text = ' '.join(subset['Text After Processing'].dropna().astype(str))
            if not text.strip(): continue

            wc = WordCloud(width=700, height=350, background_color='white', font_path=font_path, colormap='viridis').generate(text)
            plt.figure(figsize=(9,5))
            plt.imshow(wc, interpolation='bilinear')
            plt.axis('off')
            plt.title(f'{split} / {ttype}')
            add_plot_to_word(doc, plt, f'WordCloud: {split} / {ttype}')
            plt.show()

    # N-grams (Sheet 6 Collection)
    doc.add_heading('N-grams Analysis', level=1)

    all_ngrams_list = [] # List to collect all ngrams for Excel

    def plot_and_collect_ngrams(texts, split_name, type_name, n=2, top_k=10):
        vec = CountVectorizer(ngram_range=(n, n))
        try:
            bag = vec.fit_transform(texts.dropna())
            sum_words = bag.sum(axis=0)
            freqs = sorted([(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()], key=lambda x: x[1], reverse=True)[:top_k]

            # Create DF for this subset
            df_subset = pd.DataFrame(freqs, columns=['Bigram', 'Frequency'])
            df_subset['Split'] = split_name
            df_subset['Text Type'] = type_name

            # Add to collection
            all_ngrams_list.append(df_subset)

            # Plot
            plt.figure(figsize=(10,4))
            sns.barplot(x='Frequency', y='Bigram', data=df_subset, palette='crest')
            plt.title(f'Top {n}-grams: {split_name}/{type_name}')
            plt.tight_layout()
            add_plot_to_word(doc, plt, f'Top Bigrams: {split_name}/{type_name}')
            plt.show()

        except ValueError:
            pass

    for split in df_all['Split'].unique():
        for ttype in df_all[df_all['Split']==split]['Text Type'].unique():
            texts = df_all[(df_all['Split'] == split) & (df_all['Text Type'] == ttype)]['Text After Processing']
            plot_and_collect_ngrams(texts, split, ttype, n=2)

    # Combine all ngrams for Sheet 6
    full_ngram_df = pd.concat(all_ngrams_list, ignore_index=True) if all_ngrams_list else pd.DataFrame()

    # ==========================================
    # 5. Summary Metrics (Sheet 7)
    # ==========================================
    summary_metrics = pd.DataFrame({
        'Metric': ['Total Records', 'Human Texts', 'AI Texts', 'Splits', 'Avg Orig Len', 'Avg Proc Len', 'Compression'],
        'Value': [
            len(df_all),
            len(df_all[df_all['Text Type']=='Human']),
            len(df_all[df_all['Text Type']=='AI']),
            df_all["Split"].nunique(),
            round(df_all["Original Text Length"].mean(), 2),
            round(df_all["Processed Text Length"].mean(), 2),
            f'{df_all["Compression Ratio"].mean():.2%}'
        ]
    })

    # ==========================================
    # 6. Export Results (8-Sheet Excel)
    # ==========================================
    excel_filename = 'EDA_Analysis_Results.xlsx'

    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
        # Sheet 1
        stat_summary.to_excel(writer, sheet_name='Statistical Summary', index=False)
        # Sheet 2
        mean_summary.to_excel(writer, sheet_name='Global Averages', index=False)
        # Sheet 3
        lexical_means.to_excel(writer, sheet_name='Lexical Analysis', index=False)
        # Sheet 4
        keyword_summary.to_excel(writer, sheet_name='Keywords', index=False)
        # Sheet 5
        length_summary.to_excel(writer, sheet_name='Text Length', index=False)
        # Sheet 6
        full_ngram_df.to_excel(writer, sheet_name='N-grams', index=False)
        # Sheet 7
        summary_metrics.to_excel(writer, sheet_name='Summary', index=False)
        # Sheet 8
        df_all.to_excel(writer, sheet_name='Complete Data', index=False)

    word_filename = 'EDA_Analysis_Results.docx'
    doc.save(word_filename)

    print("\n" + "="*80)
    print("SUCCESS: Files Generated Successfully!")
    print(f"1. Excel: {excel_filename}")
    print(f"2. Word:  {word_filename}")
    print("="*80)

    try:
        from google.colab import files
        files.download(excel_filename)
        files.download(word_filename)
    except ImportError:
        print("Files saved locally.")
else:
    print("No data loaded.")


