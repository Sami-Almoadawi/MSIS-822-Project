
#  Table of Contents

- [Project Summary](#Project-Summary)
- [Task Description](#task-description)
- [Dataset Description](#dataset-Description)
- [System Architecture](#system-architecture)
- [Methodology](#Methodology)
- [Model Cards](#model-cards)
- [Training](#training)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Results & Analysis](#Results-&-Analysis)
- [File Structure](#file-structure)
- [References](#References)


---

#  Project Summary

This repository includes the code and tools for distinguishing between **AI-generated and human-written Arabic literature** with a hybrid approach that integrates stylistic features and deep semantic embeddings.  
The system incorporates sophisticated **Arabic preprocessing**, **sentence-transformer embeddings**, and a collection of **machine learning/deep learning classifiers** for enhanced performance.

---

#  Task Description

The assignment is defined as a binary classification problem:

 **1 — Authored by humans**
 **0 — Generated by AI**

 Each Arabic abstract is processed, encoded into embeddings, then categorised utilising one of many trained models.
 The research tackles the increasing difficulty of automatically detecting created Arabic academic literature.

---

#  Dataset Description

We use a large dataset of **41,940 Arabic research abstracts**, composed of:

- **original_abstract**: The original human-written Arabic abstract
- **{model}_generated_abstract** Machine-generated version from each model

### Dataset Statistics

| Subset | Count | Ratio |
|-------|--------|--------|
| Training | 29,358 | 70% |
| Validation | 6,291 | 15% |
| Testing | 6,291 | 15% |
| Total Samples | 41,940 | 100% |
| Classes | 0 (AI), 1 (Human) | 2

Each entry contains:

- `abstract_text`  
- `generated_by`  
- `source_split`  
- `label`  

The dataset is **balanced**, guaranteeing consistent model performance.

---
##  Traditional Machine Learning Results

| **Model**             | **Accuracy** | **Precision** | **Recall** | **F1-score** |
|----------------------|--------------|---------------|------------|---------------|
| Random Forest        | **0.978**    | **0.98**      | **0.98**   | **0.98**      |
| Logistic Regression  | 0.962        | 0.96          | 0.96       | 0.96          |
| SVM                  | 0.975        | 0.98          | 0.98       | 0.98          |
| XGBoost              | 0.969        | 0.97          | 0.95       | 0.97          |

##  Deep Learning Results

| **Model**                     | **Accuracy** | **Precision** | **Recall** | **F1-score** |
|------------------------------|--------------|---------------|------------|---------------|
| Feedforward NN + BERT (768D) | 0.8617        | 0.9437       | 0.8713     | 0.9257        |
